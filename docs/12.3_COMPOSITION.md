# Agent Composition and Orchestration

**Part of**: [Python Architecture for Aerospace Reliability](12.0_OVERVIEW.md)

---

## TL;DR

Composition operators combine agents into more complex systems while preserving reliability guarantees. Four operators are provided:
- **⊗ᵣ** (Redundant Parallel): Run N agents, vote on result
- **;f** (Fallback Sequential): Try agents until one succeeds
- **★ᵣ** (Resilient Iterative): Loop until convergence with retry
- **+ₚ** (Protected Augmentation): Wrap tools with circuit breakers

**Key Property**: All operators satisfy *closure* - composing agents produces another agent.

---

## I. Agent Fundamentals

### 1.1 Agent as Function

An agent is a function that transforms input to output with context:

```
A: Input × Context → AsyncIterator[Output]
```

**Key Properties:**
- Async streaming output via `AsyncIterator`
- Access to session state via `Context`
- Named for identification and debugging

### 1.2 Agent Hierarchy

```
Agent (Protocol)
├── LlmAgent           # Direct LLM invocation
├── ComposedAgent      # Result of composition
│   ├── ParallelAgent  # Concurrent execution
│   ├── SequentialAgent# Pipeline execution
│   └── IterativeAgent # Loop until condition
└── ToolAgent          # Tool invocation wrapper
```

---

## II. Composition Operators

### 2.1 Operator Overview

| Operator | Symbol | Type | Algebraic Properties |
|----------|--------|------|---------------------|
| Redundant Parallel | ⊗ᵣ | `(Agent, Agent, ...) → Agent` | Commutative, Associative |
| Fallback Sequential | ;f | `(Agent, Agent, ...) → Agent` | Non-commutative, Associative |
| Resilient Iterative | ★ᵣ | `(Agent, Predicate, n) → Agent` | Bounded, Resilient |
| Protected Augmentation | +ₚ | `(Agent, Tool, ...) → Agent` | Safe, Recoverable |

### 2.2 Redundant Parallel Operator (⊗ᵣ)

**Purpose**: Execute N agents in parallel, select result via voting.

```
A ⊗ᵣ B ⊗ᵣ C = RedundantParallelAgent([A, B, C], voting_strategy)
```

**Semantics:**
```python
async def execute(input, context):
    # Run all agents concurrently
    results = await asyncio.gather(*[
        a.execute(input, context) for a in agents
    ], return_exceptions=True)

    # Filter successful results
    valid_results = [r for r in results if not isinstance(r, Exception)]

    # Vote on consensus
    if len(valid_results) >= voting_strategy.min_results_required:
        return voting_strategy.vote(valid_results)
    else:
        raise NoConsensusError(results)
```

**Configuration:**
```python
@dataclass
class RedundantParallelConfig:
    agents: List[Agent]
    voting_strategy: VotingStrategy = MajorityVoting()
    min_successful: int = 2  # Minimum results for consensus
    timeout: float = 30.0    # Per-agent timeout
```

**Fault Tolerance:**
- Tolerates `⌊(n-1)/2⌋` failures with majority voting
- TMR (Triple Modular Redundancy): 3 agents, tolerates 1 failure
- 5MR: 5 agents, tolerates 2 failures

### 2.3 Fallback Sequential Operator (;f)

**Purpose**: Try agents sequentially until one succeeds.

```
A ;f B ;f C = FallbackSequentialAgent([A, B, C])
```

**Semantics:**
```python
async def execute(input, context):
    errors = []
    for agent in agents:
        try:
            async for result in agent.execute(input, context):
                yield result
            return  # Success, stop trying
        except Exception as e:
            errors.append(e)
            continue  # Try next agent

    raise RecoveryExhaustedError(levels_tried=errors)
```

**Configuration:**
```python
@dataclass
class FallbackSequentialConfig:
    agents: List[Agent]
    stop_on_success: bool = True
    collect_errors: bool = True
```

**Use Cases:**
- Primary → Secondary → Tertiary providers
- Expensive → Cheap model fallback
- Online → Cached response fallback

### 2.4 Resilient Iterative Operator (★ᵣ)

**Purpose**: Loop agent execution until convergence, with per-iteration retry.

```
★ᵣ(A, predicate, max_iterations) = ResilientIterativeAgent(A, predicate, max_iterations)
```

**Semantics:**
```python
async def execute(input, context):
    current_input = input
    for iteration in range(max_iterations):
        try:
            result = await self._execute_with_retry(current_input, context)

            if await convergence_predicate(result, context):
                return result

            current_input = transform(result)  # Prepare for next iteration

        except Exception as e:
            if iteration == max_iterations - 1:
                raise
            # Retry will handle within _execute_with_retry

    raise ConvergenceError(f"Did not converge in {max_iterations} iterations")
```

**Configuration:**
```python
@dataclass
class ResilientIterativeConfig:
    agent: Agent
    convergence_predicate: Callable[[Any, ExecutionContext], Awaitable[bool]]
    max_iterations: int = 10
    retry_config: RetryConfig = RetryConfig(max_retries=3)
    transform: Callable[[Any], Any] = identity  # Input transform between iterations
```

**Convergence Predicates:**

| Predicate | Description | Use Case |
|-----------|-------------|----------|
| `LLMJudge` | LLM evaluates if goal met | Subjective quality |
| `SchemaMatch` | Output matches schema | Structured output |
| `DiffThreshold` | Change < threshold | Optimization |
| `MaxScore` | Score >= target | Measurable goals |

### 2.5 Protected Augmentation Operator (+ₚ)

**Purpose**: Wrap tools with circuit breakers and recovery.

```
A +ₚ T₁ +ₚ T₂ = ProtectedAgent(A, [ProtectedTool(T₁), ProtectedTool(T₂)])
```

**Semantics:**
```python
class ProtectedTool:
    def __init__(self, tool: Tool, circuit_breaker: CircuitBreakerIsolation):
        self.tool = tool
        self.circuit_breaker = circuit_breaker

    async def __call__(self, *args, **kwargs):
        await self.circuit_breaker.check_request_allowed()
        try:
            result = await self.tool(*args, **kwargs)
            await self.circuit_breaker.record_success()
            return result
        except Exception as e:
            await self.circuit_breaker.record_failure()
            raise
```

**Configuration:**
```python
@dataclass
class ProtectedAugmentationConfig:
    agent: Agent
    protected_tools: List[ProtectedTool]
    fallback_tool: Optional[Tool] = None  # Used when circuit open
```

---

## III. Voting Strategies

### 3.1 Strategy Overview

| Strategy | Consensus Rule | Min Results | Use Case |
|----------|---------------|-------------|----------|
| `MajorityVoting` | > 50% agree | ⌈n/2⌉ + 1 | General purpose |
| `UnanimousVoting` | 100% agree | n | Critical decisions |
| `WeightedVoting` | Weighted majority | Configurable | Trusted agents |
| `LLMJudgeVoting` | LLM selects best | 2 | Quality-based |

### 3.2 MajorityVoting

```python
class MajorityVoting:
    def __init__(self, similarity_threshold: float = 0.9):
        self.similarity_threshold = similarity_threshold

    @property
    def min_results_required(self) -> int:
        return 2

    def vote(self, results: List[Any]) -> Any:
        # Group similar results
        groups = self._group_similar(results)

        # Find majority group
        majority_group = max(groups, key=len)

        if len(majority_group) > len(results) / 2:
            return majority_group[0]  # Return representative
        else:
            raise NoConsensusError(results, "No majority found")
```

### 3.3 WeightedVoting

```python
@dataclass
class AgentWeight:
    agent_id: str
    weight: float  # 0.0 to 1.0

class WeightedVoting:
    def __init__(self, weights: List[AgentWeight], threshold: float = 0.5):
        self.weights = {w.agent_id: w.weight for w in weights}
        self.threshold = threshold

    def vote(self, results: List[Tuple[str, Any]]) -> Any:
        # results = [(agent_id, result), ...]
        weighted_scores = defaultdict(float)

        for agent_id, result in results:
            weight = self.weights.get(agent_id, 1.0)
            weighted_scores[self._normalize(result)] += weight

        best = max(weighted_scores.items(), key=lambda x: x[1])
        if best[1] >= self.threshold * sum(self.weights.values()):
            return best[0]
        else:
            raise NoConsensusError(results)
```

### 3.4 LLMJudgeVoting

```python
class LLMJudgeVoting:
    def __init__(self, judge_agent: Agent, criteria: str):
        self.judge = judge_agent
        self.criteria = criteria

    async def vote(self, results: List[Any]) -> Any:
        prompt = f"""
        Select the best response based on: {self.criteria}

        Responses:
        {self._format_responses(results)}

        Return the index (0-based) of the best response.
        """

        judge_result = await self.judge.execute(prompt, context)
        selected_index = int(judge_result)
        return results[selected_index]
```

---

## IV. Algebraic Properties

### 4.1 Property Matrix

| Property | ⊗ᵣ | ;f | ★ᵣ | +ₚ |
|----------|----|----|----|----|
| Commutative | ✅ | ❌ | N/A | ❌ |
| Associative | ✅ | ✅ | N/A | ✅ |
| Identity | ✅ (fail-agent) | ✅ (always-fail) | ✅ (n=1) | ✅ (no tools) |
| Closure | ✅ | ✅ | ✅ | ✅ |

### 4.2 Composition Guarantees

**Theorem 10 (Operator Closure):**
> All reliability operators produce agents: `∀ op ∈ {⊗ᵣ, ;f, ★ᵣ, +ₚ}: op(agents) → Agent`

**Theorem 11 (Mixed Composition):**
> Reliability operators compose with standard operators: `(A ⊗ᵣ B) ; C` and `A ⊗ᵣ (B ; C)` are both valid.

**Theorem 9 (FDIR Composition):**
> FDIR wrapping composes: `FDIR(FDIR(A)) ≡ FDIR(A)` with combined detection strategies.

### 4.3 Composition Examples

**Example 1: TMR Research → Sequential Synthesis**
```python
# Three research agents with voting
research = A₁ ⊗ᵣ A₂ ⊗ᵣ A₃

# Sequential synthesis pipeline
synthesis = B₁ ; B₂ ; B₃

# Combined: parallel research, sequential synthesis
pipeline = research ; synthesis
```

**Example 2: Nested Fallback with TMR**
```python
# Each tier has TMR
tier1 = A₁ ⊗ᵣ A₂ ⊗ᵣ A₃
tier2 = B₁ ⊗ᵣ B₂ ⊗ᵣ B₃
tier3 = C₁ ⊗ᵣ C₂ ⊗ᵣ C₃

# Fallback across tiers
system = tier1 ;f tier2 ;f tier3
```

**Example 3: Iterative Refinement with Protected Tools**
```python
# Agent with protected search tool
agent = A +ₚ SearchTool

# Iterate until quality threshold
system = ★ᵣ(agent, quality_judge, max_iterations=5)
```

---

## V. Session State and Communication

### 5.1 Output Key Binding

Agents communicate via session state using `output_key`:

```python
agent = LlmAgent(
    name="researcher",
    output_key="research_results"  # Stores output in session
)

# Later agent accesses via template
synthesis = LlmAgent(
    name="synthesizer",
    instruction="Synthesize: {research_results}"  # Template substitution
)
```

### 5.2 Inter-Agent Communication Patterns

| Pattern | Mechanism | Use Case |
|---------|-----------|----------|
| **State Passing** | `output_key` → template | Sequential pipelines |
| **Event Streaming** | `AsyncIterator` | Real-time updates |
| **Shared Context** | `context.session_state` | Parallel coordination |
| **Tool Results** | Tool return values | Agent-tool interaction |

---

## VI. Convergence Mechanisms

### 6.1 LLM-as-Judge Pattern

```python
class LLMJudge:
    """Evaluate if agent output meets goal."""

    def __init__(self, judge_agent: Agent, goal: str):
        self.judge = judge_agent
        self.goal = goal

    async def __call__(self, output: Any, context: ExecutionContext) -> bool:
        prompt = f"""
        Goal: {self.goal}

        Output to evaluate:
        {output}

        Does this output achieve the goal? Answer only: true or false
        """

        result = await self.judge.execute(prompt, context)
        return result.lower().strip() == "true"
```

### 6.2 Convergence Parameters

| Parameter | Description | Typical Value |
|-----------|-------------|---------------|
| `max_iterations` | Hard limit on loops | 5-10 |
| `convergence_threshold` | Similarity for convergence | 0.95 |
| `patience` | Iterations without improvement | 2-3 |
| `min_improvement` | Required delta per iteration | 0.01 |

### 6.3 Convergence Detection

```python
class ConvergenceDetector:
    def __init__(
        self,
        threshold: float = 0.95,
        patience: int = 2,
        min_improvement: float = 0.01
    ):
        self.history: List[float] = []
        self.threshold = threshold
        self.patience = patience
        self.min_improvement = min_improvement

    async def check(self, score: float) -> bool:
        self.history.append(score)

        # Threshold reached
        if score >= self.threshold:
            return True

        # No improvement for patience iterations
        if len(self.history) >= self.patience:
            recent = self.history[-self.patience:]
            if max(recent) - min(recent) < self.min_improvement:
                return True  # Converged (plateau)

        return False
```

---

## Next Steps

- **Patterns**: See [12.4_PATTERNS.md](12.4_PATTERNS.md) for high-level pattern implementations
- **Integration**: See [12.5_INTEGRATION.md](12.5_INTEGRATION.md) for framework adapters
- **Code**: See [12.6_EXAMPLES.md](12.6_EXAMPLES.md) for complete implementations
